{"cells":[{"cell_type":"markdown","id":"4f310624-409c-449c-a191-6263b97df576","metadata":{"id":"4f310624-409c-449c-a191-6263b97df576"},"source":["https://keras.io/examples/audio/ctc_asr/"]},{"cell_type":"markdown","id":"8d7ae1ed-4a5b-4325-a797-54e330ae857f","metadata":{"id":"8d7ae1ed-4a5b-4325-a797-54e330ae857f"},"source":["We will evaluate the quality of the model using Word Error Rate (WER). WER is obtained by adding up the substitutions, insertions, and deletions that occur in a sequence of recognized words. Divide that number by the total number of words originally spoken. The result is the WER. To get the WER score you need to install the jiwer package. You can use the following command line:<br>\n","```shell\n","pip install jiwer\n","```"]},{"cell_type":"markdown","id":"7c25989b-6878-4bf3-9911-9360fad11bc5","metadata":{"id":"7c25989b-6878-4bf3-9911-9360fad11bc5"},"source":["References:\n","\n","- LJSpeech Dataset\n","- Speech recognition\n","- Sequence Modeling With CTC\n","- DeepSpeech2"]},{"cell_type":"markdown","id":"6ea4ae14-e9ba-48fd-82cf-193fd8514268","metadata":{"id":"6ea4ae14-e9ba-48fd-82cf-193fd8514268"},"source":["## SetUp"]},{"cell_type":"code","execution_count":5,"id":"48e471eb-bdc8-4f15-89f7-6e1221c46585","metadata":{"id":"48e471eb-bdc8-4f15-89f7-6e1221c46585","executionInfo":{"status":"ok","timestamp":1691164570071,"user_tz":-120,"elapsed":3,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_io as tfio # este es nuevo. Nosotros tenemos mp3 y no he encontrado tf.audio.decode_wav(file) para mp3\n","#Use pip install tensorflow_io NOT tensorflow-io\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","from IPython import display\n","from jiwer import wer"]},{"cell_type":"markdown","id":"75417204-787c-4867-9d97-c5d3e8fedffe","metadata":{"id":"75417204-787c-4867-9d97-c5d3e8fedffe"},"source":["## Load the CommonVoice Dataset\n","El objetivo es conseguir un dataframe de nombre 'metadata_df' y que tenga 2 columnas:\n","- 'file_name' : con el nombre de los archivos.\n","- 'normalized_transcriprion' : Con el texto de los audios\n"]},{"cell_type":"code","execution_count":10,"id":"dd9b196a-2dbe-48e4-bb17-e0f41c59bd19","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"dd9b196a-2dbe-48e4-bb17-e0f41c59bd19","executionInfo":{"status":"error","timestamp":1691165003723,"user_tz":-120,"elapsed":652,"user":{"displayName":"maria maria","userId":"09489886093533524613"}},"outputId":"49bef093-da9b-40cd-8133-75601172b4a8"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-6d3aa67cf49e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Read metadata file and parse it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmetadata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'normalized_transcription'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mjjm/anaconda3/envs/ProyectoFinalIA/Audios/es/other.csv'"]}],"source":["#data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n","#data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)\n","data_path =\"/mjjm/anaconda3/envs/ProyectoFinalIA/Audios/es\"\n","wavs_path = data_path + \"/clips/\" #los audios estan en formato mp3\n","metadata_path = data_path + \"/other.csv\"\n","\n","\n","# Read metadata file and parse it\n","metadata_df = pd.read_csv(metadata_path, sep=\",\")\n","metadata_df.rename(columns = {'path':'file_name', 'sentence': 'normalized_transcription'}, inplace = True)\n","#metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n","metadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\n","metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n","metadata_df.head(3)"]},{"cell_type":"markdown","id":"6fa076c3-8c5a-469f-9841-3df5666d3bb2","metadata":{"id":"6fa076c3-8c5a-469f-9841-3df5666d3bb2"},"source":["#### comprobaciones"]},{"cell_type":"code","execution_count":null,"id":"1fd192af-94b2-4677-8580-fe7acc51c5b8","metadata":{"id":"1fd192af-94b2-4677-8580-fe7acc51c5b8","executionInfo":{"status":"aborted","timestamp":1691164451620,"user_tz":-120,"elapsed":8,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["print(data_path)\n","print(wavs_path)\n","print(metadata_path)\n","print(metadata_df.shape)"]},{"cell_type":"markdown","id":"42015c88-7279-446c-9887-510cc5694519","metadata":{"id":"42015c88-7279-446c-9887-510cc5694519"},"source":["## Split data into trainning and validation set"]},{"cell_type":"code","execution_count":null,"id":"85f89a92-8e74-4a05-b15c-983b265daf2d","metadata":{"id":"85f89a92-8e74-4a05-b15c-983b265daf2d","executionInfo":{"status":"aborted","timestamp":1691164451621,"user_tz":-120,"elapsed":9,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["split = int(len(metadata_df) * 0.90)\n","df_train = metadata_df[:split]\n","df_val = metadata_df[split:]\n","\n","print(f\"Size of the training set: {len(df_train)}\")\n","print(f\"Size of the training set: {len(df_val)}\")"]},{"cell_type":"markdown","id":"fff906be-d468-41f8-acb2-346c986bd9f3","metadata":{"id":"fff906be-d468-41f8-acb2-346c986bd9f3"},"source":["## Preprocessing"]},{"cell_type":"markdown","id":"ac653e98-d1cd-43d0-afd3-208f0f7e031a","metadata":{"id":"ac653e98-d1cd-43d0-afd3-208f0f7e031a"},"source":["### Prepare the vocabulary"]},{"cell_type":"code","execution_count":null,"id":"ab5a813d-6530-45e5-8071-99d5d3601ee2","metadata":{"id":"ab5a813d-6530-45e5-8071-99d5d3601ee2","executionInfo":{"status":"aborted","timestamp":1691164451621,"user_tz":-120,"elapsed":8,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["# The set of characters accepted in the transcription.\n","characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n","# Mapping characters to integers\n","char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n","# Mapping integers back to original characters\n","num_to_char = keras.layers.StringLookup(\n","    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",")\n","\n","print(\n","    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n","    f\"(size ={char_to_num.vocabulary_size()})\"\n",")"]},{"cell_type":"markdown","id":"9f1bfcbf-8f47-4e1e-b2ad-5dd485bd259d","metadata":{"id":"9f1bfcbf-8f47-4e1e-b2ad-5dd485bd259d"},"source":["#### Comprobaciones. Pdte de trabajar con tensores"]},{"cell_type":"code","execution_count":null,"id":"51b2a7d0-8a05-40fb-8772-22212c7f45bb","metadata":{"id":"51b2a7d0-8a05-40fb-8772-22212c7f45bb","executionInfo":{"status":"aborted","timestamp":1691164451621,"user_tz":-120,"elapsed":8,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["print(type(char_to_num))"]},{"cell_type":"code","execution_count":null,"id":"eb0c00a2-4928-4317-89e3-afb25e777fa3","metadata":{"id":"eb0c00a2-4928-4317-89e3-afb25e777fa3","executionInfo":{"status":"aborted","timestamp":1691164451622,"user_tz":-120,"elapsed":9,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["# print(dir(keras.layers.StringLookup)) # He intentado buscar los metodos de la clase\n","msg = np.str_(['hola'])\n","msg_num = char_to_num([msg])\n","print(char_to_num([msg]))\n"]},{"cell_type":"markdown","id":"e137c66c-de8c-40aa-a9d2-1820b6571d11","metadata":{"id":"e137c66c-de8c-40aa-a9d2-1820b6571d11"},"source":["### Prepare a function to apply to each element of the dataset"]},{"cell_type":"code","execution_count":null,"id":"7f30042b-92f7-4dd5-9233-0d084567b026","metadata":{"id":"7f30042b-92f7-4dd5-9233-0d084567b026","executionInfo":{"status":"aborted","timestamp":1691164451622,"user_tz":-120,"elapsed":8,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["frame_length = 256\n","# An integer scalar Tensor. The number of samples to step.\n","frame_step = 160\n","# An integer scalar Tensor. The size of the FFT to apply.\n","# If not provided, uses the smallest power of 2 enclosing frame_length.\n","fft_length = 384\n","\n","\n","def encode_single_sample(wav_file, label):\n","    ###########################################\n","    ##  Process the Audio\n","    ##########################################\n","    # 1. Read wav file\n","    #file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n","    file = tf.io.read_file(wavs_path + wav_file)\n","    # 2. Decode the wav file\n","    #audio, _ = tf.audio.decode_wav(file)\n","    audio = tfio.audio.decode_mp3(file)\n","    audio = tf.squeeze(audio, axis=-1)\n","    # 3. Change type to float\n","    audio = tf.cast(audio, tf.float32)\n","    # 4. Get the spectrogram\n","    spectrogram = tf.signal.stft(\n","        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n","    )\n","    # 5. We only need the magnitude, which can be derived by applying tf.abs\n","    spectrogram = tf.abs(spectrogram)\n","    spectrogram = tf.math.pow(spectrogram, 0.5)\n","    # 6. normalisation\n","    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n","    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n","    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n","    ###########################################\n","    ##  Process the label\n","    ##########################################\n","    # 7. Convert label to Lower case\n","    label = tf.strings.lower(label)\n","    # 8. Split the label\n","    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n","    # 9. Map the characters in label to numbers\n","    label = char_to_num(label)\n","    # 10. Return a dict as our model is expecting two inputs\n","    return spectrogram, label"]},{"cell_type":"markdown","id":"17d00408-9cb2-4e47-9cb7-8551ef46ea37","metadata":{"id":"17d00408-9cb2-4e47-9cb7-8551ef46ea37"},"source":["## Creating Dataset objects"]},{"cell_type":"code","execution_count":null,"id":"7f5e66ab-aecb-486d-a1ab-56c5d779929a","metadata":{"id":"7f5e66ab-aecb-486d-a1ab-56c5d779929a","executionInfo":{"status":"aborted","timestamp":1691164451622,"user_tz":-120,"elapsed":8,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["batch_size = 32\n","# Define the training dataset\n","train_dataset = tf.data.Dataset.from_tensor_slices(\n","    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",")\n","train_dataset = (\n","    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n","    .padded_batch(batch_size)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")\n","\n","# Define the validation dataset\n","validation_dataset = tf.data.Dataset.from_tensor_slices(\n","    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",")\n","validation_dataset = (\n","    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n","    .padded_batch(batch_size)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")"]},{"cell_type":"markdown","id":"4f5eaf78-1f4a-442e-869c-e16f0ff08d7f","metadata":{"id":"4f5eaf78-1f4a-442e-869c-e16f0ff08d7f"},"source":["### Visualize the data"]},{"cell_type":"code","execution_count":null,"id":"592a52b3-582f-4a72-802f-bedd3b0e9251","metadata":{"scrolled":true,"id":"592a52b3-582f-4a72-802f-bedd3b0e9251","executionInfo":{"status":"aborted","timestamp":1691164451622,"user_tz":-120,"elapsed":8,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["fig = plt.figure(figsize=(8, 5))\n","for batch in train_dataset.take(1):\n","    spectrogram = batch[0][0].numpy()\n","    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n","    label = batch[1][0]\n","    # Spectrogram\n","    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n","    ax = plt.subplot(2, 1, 1)\n","    ax.imshow(spectrogram, vmax=1)\n","    ax.set_title(label)\n","    ax.axis(\"off\")\n","    # Wav\n","    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] )\n","    #audio, _ = tf.audio.decode_wav(file)\n","    audio = tfio.audio.decode_mp3(file)\n","    audio = audio.numpy()\n","    ax = plt.subplot(2, 1, 2)\n","    plt.plot(audio)\n","    ax.set_title(\"Signal Wave\")\n","    ax.set_xlim(0, len(audio))\n","    display.display(display.Audio(np.transpose(audio), rate=16000))\n","plt.show()"]},{"cell_type":"markdown","id":"33a820eb-b94e-45fe-b7fe-9617007fb061","metadata":{"id":"33a820eb-b94e-45fe-b7fe-9617007fb061"},"source":["## MODEL"]},{"cell_type":"markdown","id":"4a72bb23-bb36-4a07-b047-ab497f842eb3","metadata":{"id":"4a72bb23-bb36-4a07-b047-ab497f842eb3"},"source":["### Loss Function"]},{"cell_type":"code","execution_count":null,"id":"de6153ac-0915-4cf5-baf1-4b09f13d9df1","metadata":{"id":"de6153ac-0915-4cf5-baf1-4b09f13d9df1","executionInfo":{"status":"aborted","timestamp":1691164451623,"user_tz":-120,"elapsed":9,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["def CTCLoss(y_true, y_pred):\n","    # Compute the training-time loss value\n","    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n","    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n","    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n","\n","    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","\n","    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n","    return loss"]},{"cell_type":"markdown","id":"87443dbd-f13a-444e-b2a1-5af50eeca1bb","metadata":{"id":"87443dbd-f13a-444e-b2a1-5af50eeca1bb"},"source":["### Neuronal Network model"]},{"cell_type":"code","execution_count":null,"id":"db5ab57b-927d-45d7-bd82-a91130929b70","metadata":{"id":"db5ab57b-927d-45d7-bd82-a91130929b70","executionInfo":{"status":"aborted","timestamp":1691164451623,"user_tz":-120,"elapsed":9,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n","    \"\"\"Model similar to DeepSpeech2.\"\"\"\n","    # Model's input\n","    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n","    # Expand the dimension to use 2D CNN.\n","    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n","    # Convolution layer 1\n","    x = layers.Conv2D(\n","        filters=32,\n","        kernel_size=[11, 41],\n","        strides=[2, 2],\n","        padding=\"same\",\n","        use_bias=False,\n","        name=\"conv_1\",\n","    )(x)\n","    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n","    x = layers.ReLU(name=\"conv_1_relu\")(x)\n","    # Convolution layer 2\n","    x = layers.Conv2D(\n","        filters=32,\n","        kernel_size=[11, 21],\n","        strides=[1, 2],\n","        padding=\"same\",\n","        use_bias=False,\n","        name=\"conv_2\",\n","    )(x)\n","    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n","    x = layers.ReLU(name=\"conv_2_relu\")(x)\n","    # Reshape the resulted volume to feed the RNNs layers\n","    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n","    # RNN layers\n","    for i in range(1, rnn_layers + 1):\n","        recurrent = layers.GRU(\n","            units=rnn_units,\n","            activation=\"tanh\",\n","            recurrent_activation=\"sigmoid\",\n","            use_bias=True,\n","            return_sequences=True,\n","            reset_after=True,\n","            name=f\"gru_{i}\",\n","        )\n","        x = layers.Bidirectional(\n","            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n","        )(x)\n","        if i < rnn_layers:\n","            x = layers.Dropout(rate=0.5)(x)\n","    # Dense layer\n","    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n","    x = layers.ReLU(name=\"dense_1_relu\")(x)\n","    x = layers.Dropout(rate=0.5)(x)\n","    # Classification layer\n","    output = layers.Dense(units=output_dim + 1, activation=\"softmax\", name=\"dense2\")(x)\n","    # Model\n","    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n","    # Optimizer\n","    opt = keras.optimizers.Adam(learning_rate=1e-4)\n","    # Compile the model and return\n","    model.compile(optimizer=opt, loss=CTCLoss)\n","    return model\n","\n","\n","# Get the model\n","model = build_model(\n","    input_dim=fft_length // 2 + 1,\n","    output_dim=char_to_num.vocabulary_size(),\n","    rnn_units=512,\n",")\n","model.summary(line_length=110)"]},{"cell_type":"markdown","id":"5b8a2caa-d559-474f-a04d-a6137a7849e9","metadata":{"id":"5b8a2caa-d559-474f-a04d-a6137a7849e9"},"source":["## TRAINING AND EVALUATING"]},{"cell_type":"code","execution_count":null,"id":"949e1d9b-811e-40ff-93fe-06b7f6728396","metadata":{"id":"949e1d9b-811e-40ff-93fe-06b7f6728396","executionInfo":{"status":"aborted","timestamp":1691164451623,"user_tz":-120,"elapsed":9,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n","    # Use greedy search. For complex tasks, you can use beam search\n","    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n","    # Iterate over the results and get back the text\n","    output_text = []\n","    for result in results:\n","        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n","        output_text.append(result)\n","    return output_text\n","\n","\n","# A callback class to output a few transcriptions during training\n","class CallbackEval(keras.callbacks.Callback):\n","    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n","\n","    def __init__(self, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        predictions = []\n","        targets = []\n","        for batch in self.dataset:\n","            X, y = batch\n","            batch_predictions = model.predict(X)\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            predictions.extend(batch_predictions)\n","            for label in y:\n","                label = (\n","                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n","                )\n","                targets.append(label)\n","        wer_score = wer(targets, predictions)\n","        print(\"-\" * 100)\n","        print(f\"Word Error Rate: {wer_score:.4f}\")\n","        print(\"-\" * 100)\n","        for i in np.random.randint(0, len(predictions), 2):\n","            print(f\"Target    : {targets[i]}\")\n","            print(f\"Prediction: {predictions[i]}\")\n","            print(\"-\" * 100)"]},{"cell_type":"markdown","id":"e0307aef-27f8-4c7a-b071-68892e9e3e6d","metadata":{"id":"e0307aef-27f8-4c7a-b071-68892e9e3e6d"},"source":["Start training process"]},{"cell_type":"code","execution_count":null,"id":"0b505475-5d3d-4bed-9573-5355a2c7bf24","metadata":{"id":"0b505475-5d3d-4bed-9573-5355a2c7bf24","executionInfo":{"status":"aborted","timestamp":1691164451623,"user_tz":-120,"elapsed":9,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":["# Define the number of epochs.\n","epochs = 1\n","# Callback function to check transcription on the val set.\n","validation_callback = CallbackEval(validation_dataset)\n","# Train the model\n","history = model.fit(\n","    train_dataset,\n","    validation_data=validation_dataset,\n","    epochs=epochs,\n","    callbacks=[validation_callback],\n",")"]},{"cell_type":"code","execution_count":null,"id":"d59895c8-846c-441a-b83e-15163d96f9a0","metadata":{"id":"d59895c8-846c-441a-b83e-15163d96f9a0","executionInfo":{"status":"aborted","timestamp":1691164451624,"user_tz":-120,"elapsed":9,"user":{"displayName":"maria maria","userId":"09489886093533524613"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}